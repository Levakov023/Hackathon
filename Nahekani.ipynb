{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":50329,"databundleVersionId":5897713,"sourceType":"competition"},{"sourceId":5797705,"sourceType":"datasetVersion","datasetId":3192265},{"sourceId":5870250,"sourceType":"datasetVersion","datasetId":3374819},{"sourceId":5876050,"sourceType":"datasetVersion","datasetId":3377680}],"dockerImageVersionId":30461,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip uninstall opencv-python-headless -y \n\n!pip install opencv-python --upgrade","metadata":{"execution":{"iopub.status.busy":"2023-06-08T17:08:21.553207Z","iopub.execute_input":"2023-06-08T17:08:21.554337Z","iopub.status.idle":"2023-06-08T17:08:37.482724Z","shell.execute_reply.started":"2023-06-08T17:08:21.554291Z","shell.execute_reply":"2023-06-08T17:08:37.481143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install pycocotools","metadata":{"execution":{"iopub.status.busy":"2023-06-08T17:08:37.488027Z","iopub.execute_input":"2023-06-08T17:08:37.488435Z","iopub.status.idle":"2023-06-08T17:08:50.311087Z","shell.execute_reply.started":"2023-06-08T17:08:37.488396Z","shell.execute_reply":"2023-06-08T17:08:50.30963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torchvision\nimport torchvision.datasets as dset\nimport torchvision.transforms as transforms\nimport os\nimport torch\nimport torch.utils.data\nfrom PIL import Image\nfrom pycocotools.coco import COCO\nimport matplotlib.pyplot as plt\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection import FasterRCNN_ResNet50_FPN_Weights\nimport pprint\nimport random\npp = pprint.PrettyPrinter(indent=1)\npath2data=\"/kaggle/input/levi9-hack9-2023/train\" # Add corect path\npath2json=\"/kaggle/input/train-test/mata-train.json\" # Add corect path","metadata":{"execution":{"iopub.status.busy":"2023-06-08T17:08:50.313346Z","iopub.execute_input":"2023-06-08T17:08:50.31384Z","iopub.status.idle":"2023-06-08T17:08:50.324478Z","shell.execute_reply.started":"2023-06-08T17:08:50.313782Z","shell.execute_reply":"2023-06-08T17:08:50.322875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Reading the data ","metadata":{}},{"cell_type":"code","source":"coco_train = dset.CocoDetection(root = path2data,\n                                annFile = path2json,\n                                transform = transforms.ToTensor())","metadata":{"execution":{"iopub.status.busy":"2023-06-08T17:08:50.326164Z","iopub.execute_input":"2023-06-08T17:08:50.326584Z","iopub.status.idle":"2023-06-08T17:08:50.362988Z","shell.execute_reply.started":"2023-06-08T17:08:50.326535Z","shell.execute_reply":"2023-06-08T17:08:50.36181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Number of samples: ', len(coco_train))","metadata":{"execution":{"iopub.status.busy":"2023-06-08T17:08:50.366965Z","iopub.execute_input":"2023-06-08T17:08:50.367646Z","iopub.status.idle":"2023-06-08T17:08:50.373369Z","shell.execute_reply.started":"2023-06-08T17:08:50.36761Z","shell.execute_reply":"2023-06-08T17:08:50.372101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img, target=coco_train[0]\nprint (img.size)\nprint(target)","metadata":{"execution":{"iopub.status.busy":"2023-06-08T17:08:50.374952Z","iopub.execute_input":"2023-06-08T17:08:50.376135Z","iopub.status.idle":"2023-06-08T17:08:50.440038Z","shell.execute_reply.started":"2023-06-08T17:08:50.37609Z","shell.execute_reply":"2023-06-08T17:08:50.43883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class myCocoDataset(torch.utils.data.Dataset):\n    def __init__(self, root, annotation, transforms=None):\n        self.root = root\n        self.transforms = transforms\n        self.coco = COCO(annotation)\n        self.ids = list(sorted(self.coco.imgs.keys()))\n\n    def __getitem__(self, index):\n        # Own coco file\n        coco = self.coco\n        # Image ID\n        img_id = self.ids[index]\n        # List: get annotation id from coco\n        ann_ids = coco.getAnnIds(imgIds=img_id)\n        # Dictionary: target coco_annotation file for an image\n        coco_annotation = coco.loadAnns(ann_ids)\n        # path for input image\n        path = coco.loadImgs(img_id)[0][\"file_name\"]\n        # open the input image\n        img = Image.open(os.path.join(self.root, path))\n        # number of objects in the image\n        num_objs = len(coco_annotation)\n\n        # Bounding boxes for objects\n        # In coco format, bbox = [xmin, ymin, width, height]\n        # In pytorch, the input should be [xmin, ymin, xmax, ymax]\n        boxes = []\n        area = 0 \n        for i in range(num_objs):\n            xmin = coco_annotation[i][\"bbox\"][0]\n            ymin = coco_annotation[i][\"bbox\"][1]\n            xmax = xmin + coco_annotation[i][\"bbox\"][2]\n            ymax = ymin + coco_annotation[i][\"bbox\"][3]\n            area += (xmax-xmin)*(ymax-ymin)\n            boxes.append([xmin, ymin, xmax, ymax])\n        if num_objs == 0:\n            boxes = torch.zeros((0, 4), dtype=torch.float32)\n        else:\n            boxes = torch.as_tensor(boxes, dtype=torch.float32)\n        # Labels (In my case, I only one class: target class or background)\n        labels = torch.ones((num_objs,), dtype=torch.int64)\n        # Tensorise img_id\n        img_id = torch.tensor([img_id])\n        # Size of bbox (Rectangular)\n        areas = []\n        for i in range(num_objs):\n            areas.append(coco_annotation[i][\"area\"])\n        area = torch.as_tensor(area, dtype=torch.float32)\n        # Iscrowd\n        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n\n        # Annotation is in dictionary format\n        my_annotation = {}\n        my_annotation[\"boxes\"] = boxes\n        my_annotation[\"labels\"] = labels\n        my_annotation[\"image_id\"] = img_id\n        my_annotation[\"area\"] = area\n        my_annotation[\"iscrowd\"] = iscrowd\n\n        if self.transforms is not None:\n            img = self.transforms(img)\n\n        return img, my_annotation\n\n    def __len__(self):\n        return len(self.ids)\n\n\n# In my case, just added ToTensor\ndef get_transform():\n    custom_transforms = []\n    custom_transforms.append(torchvision.transforms.ToTensor())\n    return torchvision.transforms.Compose(custom_transforms)\n\n\n# collate_fn needs for batch\ndef collate_fn(batch):\n    return tuple(zip(*batch))\n\n\ndef get_model_instance_segmentation(num_classes):\n    # load an instance segmentation model pre-trained pre-trained on COCO\n    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT, progress=True)\n    # get number of input features for the classifier\n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n    # replace the pre-trained head with a new one\n    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2023-06-08T17:08:50.443726Z","iopub.execute_input":"2023-06-08T17:08:50.444112Z","iopub.status.idle":"2023-06-08T17:08:50.468524Z","shell.execute_reply.started":"2023-06-08T17:08:50.444066Z","shell.execute_reply":"2023-06-08T17:08:50.467237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Config parametrii ","metadata":{}},{"cell_type":"code","source":"train_batch_size = 2\nmomentum = 0.9\nweight_decay = 0.005\n\n# Params for dataloader\ntrain_shuffle_dl = True\nnum_workers_dl = 2\n\n# Params for training\n# Two classes; Only target class or background\nnum_classes = 2\nnum_epochs = 2\n\nlr = 0.007\n","metadata":{"execution":{"iopub.status.busy":"2023-06-08T17:08:50.470283Z","iopub.execute_input":"2023-06-08T17:08:50.470905Z","iopub.status.idle":"2023-06-08T17:08:50.47894Z","shell.execute_reply.started":"2023-06-08T17:08:50.470856Z","shell.execute_reply":"2023-06-08T17:08:50.477514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training ","metadata":{}},{"cell_type":"code","source":"print(\"Torch version:\", torch.__version__)\n\n# create own Dataset\nmy_dataset = myCocoDataset(\n    root=path2data, annotation=path2json, transforms=get_transform()\n)\n\n# own DataLoader\ndata_loader = torch.utils.data.DataLoader(\n    my_dataset,\n    batch_size=train_batch_size,\n    shuffle=train_shuffle_dl,\n    num_workers=num_workers_dl,\n    collate_fn=collate_fn\n)\n\n\n# select device (whether GPU or CPU)\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n\n# DataLoader is iterable over Dataset\nfor imgs, annotations in data_loader:\n    imgs = list(img.to(device) for img in imgs)\n    annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]\n\n\nmodel = get_model_instance_segmentation(num_classes)\n\n# move model to the right device\nmodel.to(device)\n\n# parameters\nparams = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.SGD(\n    params, lr=lr, momentum=momentum, weight_decay=weight_decay\n)\n\nlen_dataloader = len(data_loader)\n\n# Training\nfor epoch in range(num_epochs):\n    print(f\"Epoch: {epoch}/{num_epochs}\")\n    model.train()\n    i = 0\n    for imgs, annotations in data_loader:\n        i += 1\n        imgs = list(img.to(device) for img in imgs)\n        annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]\n        loss_dict = model(imgs, annotations)\n        losses = sum(loss for loss in loss_dict.values())\n\n        optimizer.zero_grad()\n        losses.backward()\n        optimizer.step()\n\n        print(f\"Iteration: {i}/{len_dataloader}, Loss: {losses}\")","metadata":{"execution":{"iopub.status.busy":"2023-06-08T17:08:50.481129Z","iopub.execute_input":"2023-06-08T17:08:50.481581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.eval()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluation njihov, ovo mi ne treba","metadata":{}},{"cell_type":"code","source":"# from PIL import Image, ImageDraw\n# sample_image_path = '/kaggle/input/levi9-hack9-2023/train/046.jpg'\n# sample_image = Image.open(sample_image_path)\n# sample_image","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# transformed_img = torchvision.transforms.transforms.ToTensor()(sample_image)\n\n# result = model([transformed_img.to(device)])\n# result","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# logo_id = 1\n# logo_boxes = [x.cpu().detach().numpy().tolist() for i, x in enumerate(result[0]['boxes']) if result[0]['labels'][i] == logo_id]\n# logo_boxes\n\n# # obrisi ovo smanjivanje\n# # logo_boxes = [logo_boxes[0],logo_boxes[1],logo_boxes[2]]\n# # logo_boxes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sample_image_annotated = sample_image.copy()\n\n# img_bbox = ImageDraw.Draw(sample_image_annotated)\n\n\n \n# for bbox in logo_boxes:\n#     x1, x2, x3, x4 = map(int, bbox)\n#     print(x1, x2, x3, x4)\n#     img_bbox.rectangle([x1, x2, x3, x4], outline=\"red\") \n\n# sample_image_annotated\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Njihove funkcije trebaju posle za submition","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom itertools import groupby\nimport pycocotools._mask as _mask\ndef mask_to_rle(mask):\n    \"\"\"\n    params:  mask - numpy array\n    returns: run-length encoding string (pairs of start & length of encoding)\n    \"\"\"\n    # turn a n-dimensional array into a 1-dimensional series of pixels\n    # for example:\n    #     [[1. 1. 0.]\n    #      [0. 0. 0.]   --> [1. 1. 0. 0. 0. 0. 1. 0. 0.]\n    #      [1. 0. 0.]]\n    flat = mask.flatten()\n    \n    # we find consecutive sequences by overlaying the mask\n    # on a version of itself that is displaced by 1 pixel\n    # for that, we add some padding before slicing\n    padded = np.concatenate([[0], flat, [0]])\n    \n    # this returns the indices where the sliced arrays differ\n    runs = np.where(padded[1:] != padded[:-1])[0] \n    # indexes start at 0, pixel numbers start at 1\n    runs += 1\n\n    # every uneven element represents the start of a new sequence\n    # every even element is where the run comes to a stop\n    # subtract the former from the latter to get the length of the run\n    runs[1::2] -= runs[0::2]\n \n    # convert the array to a string\n    return ' '.join(str(x) for x in runs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def rle_to_mask(lre, shape=(1181, 1772)):\n    '''\n    params:  rle   - run-length encoding string (pairs of start & length of encoding)\n             shape - (width,height) of numpy array to return \n    \n    returns: numpy array with dimensions of shape parameter\n    '''    \n    # the incoming string is space-delimited\n    runs = np.asarray([int(run) for run in lre.split(' ')])\n    \n    # we do the same operation with the even and uneven elements, but this time with addition\n    runs[1::2] += runs[0::2]\n    # pixel numbers start at 1, indexes start at 0\n    runs -= 1\n    \n    # extract the starting and ending indeces at even and uneven intervals, respectively\n    run_starts, run_ends = runs[0::2], runs[1::2]\n    \n    # build the mask\n    h,w  = shape\n    mask = np.zeros(h*w, dtype=np.uint8)\n    for start, end in zip(run_starts, run_ends):\n        mask[start:end] = 255\n    \n    # transform the numpy array from flat to the original image shape\n    return mask.reshape(shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\ndef prediction_to_rle(image_path, model):\n    columns = 2\n    real_image = Image.open(image_path)\n    images = []\n    images.append(real_image)\n    w, h = real_image.size\n    transformed_img = torchvision.transforms.transforms.ToTensor()(real_image)\n    prediction = model([transformed_img.to(device)])\n    logo_id = 1\n    logo_boxes = [x.cpu().detach().numpy().tolist() for i, x in enumerate(prediction[0]['boxes']) if prediction[0]['labels'][i] == logo_id]\n    \n    scores = prediction[0]['scores']\n    \n    logo_boxes = [box for indexx, box in enumerate(logo_boxes) if scores[indexx] > 0.65 ] # scoreThresh\n    \n    rle = None\n    if len(logo_boxes) > 0:\n        mask = np.zeros((h, w), np.uint8)\n        real_img_bbox = ImageDraw.Draw(real_image)\n        for bbox in logo_boxes:\n            x1, x2, x3, x4 = map(int, bbox)\n            real_img_bbox.rectangle([x1, x2, x3, x4], outline=\"red\")\n            mask[x2:x4,x1:x3] = 255\n        rle = mask_to_rle(mask)\n    \n    return rle","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def image_to_binary(image_path, bounding_boxes):\n    real_image = Image.open(image_path)\n    w, h = real_image.size\n    mask = np.zeros((h, w), np.uint8)\n    real_img_bbox = ImageDraw.Draw(real_image)\n    for bbox in bounding_boxes:\n        x1, x2, x3, x4 = map(int, bbox)\n        real_img_bbox.rectangle([x1, x2, x3, x4], outline=\"red\")\n        mask[x2:x4,x1:x3] = 255\n        \n    return mask","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def image_to_binary(image_tensor, bounding_boxes):\n    tranformation = transforms.ToPILImage()\n    real_image = tranformation(image_tensor)\n    plt.imshow(real_image)\n    w, h = real_image.size\n    mask = np.zeros((h, w), np.uint8)\n    real_img_bbox = ImageDraw.Draw(real_image)\n    for bbox in bounding_boxes:\n        #print(bbox)\n        x1, x2, x3, x4 = map(int, bbox)\n        real_img_bbox.rectangle([x1, x2, x3, x4], outline=\"red\")\n        mask[x2:x4,x1:x3] = 255\n        \n    return mask","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# OVO JE SUBMIT","metadata":{}},{"cell_type":"code","source":"# import pandas as pd\n# import os\n# test_images_path = '/kaggle/input/levi9-hack9-2023/test'\n# solution = []\n# for image_path in os.listdir(test_images_path):\n#     rle = prediction_to_rle(test_images_path+'/'+image_path, model)\n#     solution.append({'img': image_path, 'pixels': rle})\n    \n\n# df = pd.DataFrame(solution)\n# df.to_csv('sub-novi2.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Mata funkcija ovo mi ne treba ja msm","metadata":{}},{"cell_type":"code","source":"# from collections import namedtuple\n# Rectangle = namedtuple('Rectangle', 'xmin ymin xmax ymax')\n\n# ra = Rectangle(3., 3., 5., 5.)\n# rb = Rectangle(1., 1., 4., 3.5)\n# # intersection here is (3, 3, 4, 3.5), or an area of 1*.5=.5\n\n# # from: \n# # https://stackoverflow.com/questions/27152904/calculate-overlapped-area-between-two-rectangles\n# def area(a, b):  # returns 0 if rectangles don't intersect\n#     dx = min(a.xmax, b.xmax) - max(a.xmin, b.xmin)\n#     dy = min(a.ymax, b.ymax) - max(a.ymin, b.ymin)\n    \n#     if (dx>=0) and (dy>=0):\n#         return dx*dy\n    \n#     return 0\n\n# print(area(ra, rb))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Moje funckija za pozivanje modela","metadata":{}},{"cell_type":"code","source":"def bboxToCocoBbox(bbox):\n    bbox[2] = bbox[2] - bbox[0]\n    bbox[3] = bbox[3] - bbox[1]\n\ndef model_predict(image_path, model):\n    real_image = Image.open(image_path)\n    images = []\n    images.append(real_image)\n    w, h = real_image.size\n    transformed_img = torchvision.transforms.transforms.ToTensor()(real_image)\n    prediction = model([transformed_img.to(device)])\n    \n    scores = prediction[0]['scores'].tolist()\n    \n    logo_id = 1\n    imageBboxes = [x.cpu().detach().numpy().tolist() for i, x in enumerate(prediction[0]['boxes']) if prediction[0]['labels'][i] == logo_id]\n    \n    for oneBbox in imageBboxes:\n        bboxToCocoBbox(oneBbox)\n    \n    \n    return imageBboxes, scores","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# slikaputanja = '/kaggle/input/levi9-hack9-2023/train/041.jpg'\n\n# bbbboxess  = model_predict(slikaputanja, model)\n\n# print(bbbboxess)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Funkcije za racunanje metrika","metadata":{}},{"cell_type":"code","source":"def calculate_metrics_simple(ground_truth, predictions, iou_threshold):\n    \"\"\"\n    Calculate metrics from COCO format annotations using IoU.\n\n    Args:\n        ground_truth (list): Ground truth annotations in COCO format.\n        predictions (list): Predicted annotations in COCO format.\n        iou_threshold (float): IoU threshold for matching predictions with ground truth.\n\n    Returns:\n        float: Precision value.\n        float: Recall value.\n        float: F1-score value.\n    \"\"\"\n    true_positives = 0\n    false_positives = 0\n    false_negatives = 0\n\n    for pred in predictions:\n        pred_bbox = pred[\"bbox\"]\n        pred_category = pred[\"category_id\"]\n\n        pred_matched = False\n\n        for gt in ground_truth:\n            gt_bbox = gt[\"bbox\"]\n            gt_category = gt[\"category_id\"]\n\n            iou = calculate_iou(pred_bbox, gt_bbox)\n\n            if iou >= iou_threshold and pred_category == gt_category:\n                true_positives += 1\n                pred_matched = True\n                break\n\n        if not pred_matched:\n            false_positives += 1\n\n    false_negatives = len(ground_truth) - true_positives\n\n\n    precision = true_positives / (true_positives + false_positives)\n    recall = true_positives / (true_positives + false_negatives)\n    \n    if precision == 0 and recall ==0:\n        f1_score = 0\n    else:\n        f1_score = 2 * (precision * recall) / (precision + recall)\n    \n    \n    print('TP')\n    print(true_positives)\n    print('FP')\n    print(false_positives)\n    print('FN')\n    print(false_negatives)\n\n    return precision, recall, f1_score\n\ndef calculate_iou(bbox1, bbox2):\n    \"\"\"\n    Calculate Intersection over Union (IoU) between two bounding boxes.\n\n    Args:\n        bbox1 (list): Bounding box coordinates [x, y, width, height].\n        bbox2 (list): Bounding box coordinates [x, y, width, height].\n\n    Returns:\n        float: IoU value.\n    \"\"\"\n    x1, y1, w1, h1 = bbox1\n    x2, y2, w2, h2 = bbox2\n\n    area1 = w1 * h1\n    area2 = w2 * h2\n\n    intersection_x = max(0, min(x1 + w1, x2 + w2) - max(x1, x2))\n    intersection_y = max(0, min(y1 + h1, y2 + h2) - max(y1, y2))\n    intersection_area = intersection_x * intersection_y\n\n    union_area = area1 + area2 - intersection_area\n\n    iou = intersection_area / union_area\n\n    return iou\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Proba da li funkcije za metrike rade","metadata":{}},{"cell_type":"code","source":"# import json\n\n# truth_file = '/kaggle/input/coco-jsons/coco-truth.json'\n# pred_file = '/kaggle/input/coco-jsons/coco-pred.json'\n# iou_thresh = 0.5\n\n# # Load predictions JSON file into a Python object\n# with open(pred_file, 'r') as f:\n#     predd = json.load(f)\n\n# with open(truth_file, 'r') as f:\n#     truthh = json.load(f)\n    \n\n# precision, recall, f1_score = calculate_metrics_simple(truthh['annotations'], predd['annotations'], iou_thresh)\n\n# print()\n# print()\n# print(precision)\n# print(recall)\n# print(f1_score)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Zapamti\n\n## TP - broj pogodjenih logoa\n## FN - broj logoa koji postoje a nismo ih pogodili\n## FP - broj nasih izmisljenih logoa","metadata":{}},{"cell_type":"markdown","source":"# Broj slika sa logom","metadata":{}},{"cell_type":"code","source":"# inputt_path = '/kaggle/input/levi9-hack9-2023/train.json'\n\n# with open(inputt_path, 'r') as f:\n#     inputt = json.load(f)\n    \n# numOfImages = len(inputt['images'])\n\n# print(\"Broj ukupnih slika\")\n# print(numOfImages)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# imagesWithLogo = []\n\n# for image in inputt['images']:\n#     logosInThisImage = sum(annotation['image_id'] == image['id']   for annotation in inputt['annotations'])\n    \n#     if logosInThisImage > 0:\n#         imagesWithLogo.append(image)\n        \n# print(\"Broj slika sa logom\")\n# print(len(imagesWithLogo))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# imagesNoLogo = []\n\n# for image in inputt['images']:\n#     logosInThisImage = sum(annotation['image_id'] == image['id']   for annotation in inputt['annotations'])\n    \n#     if logosInThisImage == 0:\n#         imagesNoLogo.append(image)\n        \n# print(\"Broj slika bez logom\")\n# print(len(imagesNoLogo))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def diffArray(arr1, arr2):\n    diffArr = []\n    \n    for image1 in arr1:\n        occurences = sum(image2['id'] == image1['id']  for image2 in arr2 )\n        \n        if occurences == 0:\n            diffArr.append(image1)\n\n    return diffArr","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# print(\"With logo\")\n# print(len(imagesWithLogo))\n# print(\"No logo\")\n# print(len(imagesNoLogo))\n# print()\n\n\n# numTrainWithLogos = int(0.75 * len(imagesWithLogo))\n# numTrainNoLogos = int(0.75 * len(imagesNoLogo))\n\n\n# trainImagesWithLogo = random.sample(imagesWithLogo, numTrainWithLogos)\n# trainImagesNoLogo = random.sample(imagesNoLogo, numTrainNoLogos)\n\n# testImagesWithLogo = diffArray(imagesWithLogo, trainImagesWithLogo)\n# testImagesNoLogo = diffArray(imagesNoLogo, trainImagesNoLogo)\n\n\n# print(\"Train logo\")\n# print(len(trainImagesWithLogo))\n# print(\"Train no logo\")\n# print(len(trainImagesNoLogo))\n# print()\n\n# print(\"Test logo\")\n# print(len(testImagesWithLogo))\n# print(\"Test no logo\")\n# print(len(testImagesNoLogo))\n# print()\n\n\n\n# trainImages = trainImagesWithLogo + trainImagesNoLogo\n# testImages = testImagesWithLogo + testImagesNoLogo\n\n# print(\"Train\")\n# print(len(trainImages))\n# print(\"test\")\n# print(len(testImages))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def idExistsInImageArray(id, imageArray):\n    num = sum(image['id'] == id for image in imageArray)\n    \n    if (num > 0):\n        return True\n    \n    return False","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# allAnnotations = inputt['annotations']\n\n# print(\"All annotations\")\n# print(len(allAnnotations))\n# print()\n\n# trainAnnotations = [annotation for annotation in allAnnotations if idExistsInImageArray(annotation['image_id'],trainImages)]\n# testAnnotations = [annotation for annotation in allAnnotations if idExistsInImageArray(annotation['image_id'],testImages)]\n\n# print(\"Train annotations\")\n# print(len(trainAnnotations))\n# print()\n\n# print(\"Test annotations\")\n# print(len(testAnnotations))\n# print()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import copy\n\n# trainInputt = copy.deepcopy(inputt)\n# testInputt = copy.deepcopy(inputt)\n\n# trainInputt['images'] = trainImages;\n# trainInputt['annotations'] = trainAnnotations;\n\n# testInputt['images'] = testImages;\n# testInputt['annotations'] = testAnnotations;\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(len(trainInputt['images']))\n# print(len(testInputt['images']))\n\n# print(len(trainInputt['annotations']))\n# print(len(testInputt['annotations']))\n\n# trainInputtJson = json.dumps(trainInputt)\n# testInputtJson = json.dumps(testInputt)\n\n# # jsonFile1 = open(\"mata-train.json\", \"w\")\n# # jsonFile1.write(trainInputtJson)\n# # jsonFile1.close()\n\n# # jsonFile1 = open(\"mata-test.json\", \"w\")\n# # jsonFile1.write(testInputtJson)\n# # jsonFile1.close()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# myTrainPath = '/kaggle/input/train-test/mata-train.json'\n# myTestPath = '/kaggle/input/train-test/mata-test.json'\n\n# with open(myTrainPath, 'r') as f:\n#     myTrain = json.load(f)\n\n# with open(myTestPath, 'r') as f:\n#     myTest = json.load(f)\n\n# print(len(myTrain['images']))\n# print(len(myTrain['annotations']))\n\n# print()\n# print(len(myTest['images']))\n# print(len(myTest['annotations']))\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Pusti model","metadata":{}},{"cell_type":"code","source":"realTruthPath = '/kaggle/input/train-test/mata-test.json'\n\nwith open(realTruthPath, 'r') as f:\n    realTruth = json.load(f)\n\ntruthImages = realTruth['images']\ntruthAnnotations = realTruth['annotations']\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predAnnotations = []\n\nfor i, image in enumerate(truthImages):\n    \n    imagePath = '/kaggle/input/levi9-hack9-2023/train/' + image['file_name']\n    # run model for this image name and get bboxes\n    imageBboxes, scores = model_predict(imagePath, model)\n    \n#     print('Boxes:')\n#     pp.pprint(imageBboxes)\n#     print()\n    \n    for boxIndex, oneBbox in enumerate(imageBboxes):\n        # get score\n        score = 0.8\n        \n        oneAnnotation = {\n                  \"id\": len(predAnnotations)+1,\n                  \"image_id\": image['id'],\n                  \"category_id\": 1,\n                  \"bbox\": oneBbox,\n                  \"score\": scores[boxIndex]\n                }\n        \n        predAnnotations.append(oneAnnotation)\n\n# print()\n# print(\"Annotations:\")\n# pp.pprint(predAnnotations)\n\n\n\nprint(\"Done\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# remove all annotations with score less than scoreThresh\nscoreThresh = 0.65\n\nscoredPredAnnotations = [annotation for annotation in predAnnotations if annotation['score'] >= scoreThresh]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Izracunaj metrike","metadata":{}},{"cell_type":"code","source":"iou_thresh = 0.5\nprecision, recall, f1_score = calculate_metrics_simple(truthAnnotations, scoredPredAnnotations, iou_thresh)\n\nprint()\nprint(precision)\nprint(recall)\nprint(f1_score)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import itertools\n\n# batch = [2, 3, 4]\n# momentum = [0.8, 0.9, 0.95]\n# wDecay = [0.01, 0.005, 0.001]\n\n# allParams = [batch, momentum, wDecay]\n\n# # Generate all combinations of parameters\n# combinations = list(itertools.product(*allParams))\n\n# # Run function for each combination\n\n# for params in combinations:\n#     print(f\"{params}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}}]}